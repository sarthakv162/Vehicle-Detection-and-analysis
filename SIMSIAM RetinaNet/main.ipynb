{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimSiam + RetinaNet ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.models.detection import RetinaNet\n",
    "from torchvision.models.detection.backbone_utils import BackboneWithFPN\n",
    "from torchvision.models._utils import IntermediateLayerGetter\n",
    "import torchvision.models as tv_models\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T14:09:15.207985Z",
     "iopub.status.busy": "2025-06-14T14:09:15.207155Z",
     "iopub.status.idle": "2025-06-14T14:09:18.996301Z",
     "shell.execute_reply": "2025-06-14T14:09:18.995571Z",
     "shell.execute_reply.started": "2025-06-14T14:09:15.207947Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (2.0.8)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools) (3.7.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools) (1.26.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (11.1.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.9.0.post0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->pycocotools) (2.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pycocotools) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pycocotools) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->pycocotools) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->pycocotools) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->pycocotools) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T14:09:18.998108Z",
     "iopub.status.busy": "2025-06-14T14:09:18.997852Z",
     "iopub.status.idle": "2025-06-14T14:09:22.904348Z",
     "shell.execute_reply": "2025-06-14T14:09:22.903646Z",
     "shell.execute_reply.started": "2025-06-14T14:09:18.998084Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yoco\n",
      "  Downloading yoco-1.2.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting ruamel.yaml (from yoco)\n",
      "  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->yoco)\n",
      "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Downloading yoco-1.2.0-py3-none-any.whl (6.4 kB)\n",
      "Downloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ruamel.yaml.clib, ruamel.yaml, yoco\n",
      "Successfully installed ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 yoco-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install yoco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converstion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T14:18:03.757544Z",
     "iopub.status.busy": "2025-06-14T14:18:03.756886Z",
     "iopub.status.idle": "2025-06-14T14:18:13.927256Z",
     "shell.execute_reply": "2025-06-14T14:18:13.926501Z",
     "shell.execute_reply.started": "2025-06-14T14:18:03.757485Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 536/536 [00:08<00:00, 65.65it/s]\n",
      "100%|██████████| 90/90 [00:01<00:00, 58.10it/s]\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "def yolo_to_coco(yolo_dir, output_json_path):\n",
    "    image_dir = os.path.join(yolo_dir, 'images')\n",
    "    label_dir = os.path.join(yolo_dir, 'labels')\n",
    "    images = []\n",
    "    annotations = []\n",
    "    ann_id = 0\n",
    "    image_id = 0\n",
    "    categories = [{'id': 1, 'name': 'vehicle'}]\n",
    "\n",
    "    for img_name in tqdm(sorted(os.listdir(image_dir))):\n",
    "        if not img_name.endswith(('.jpg', '.png', '.jpeg')):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "        label_path = os.path.join(label_dir, os.path.splitext(img_name)[0] + '.txt')\n",
    "        if not os.path.exists(label_path):\n",
    "            continue\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        w, h = img.size\n",
    "        images.append({'id': image_id, 'width': w, 'height': h, 'file_name': img_name})\n",
    "\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                cls, xc, yc, bw, bh = map(float, line.strip().split())\n",
    "                x = (xc - bw / 2) * w\n",
    "                y = (yc - bh / 2) * h\n",
    "                box_w = bw * w\n",
    "                box_h = bh * h\n",
    "                if box_w <= 0 or box_h <= 0: continue\n",
    "                annotations.append({\n",
    "                    'id': ann_id,\n",
    "                    'image_id': image_id,\n",
    "                    'category_id': 1,\n",
    "                    'bbox': [x, y, box_w, box_h],\n",
    "                    'area': box_w * box_h,\n",
    "                    'iscrowd': 0\n",
    "                })\n",
    "                ann_id += 1\n",
    "        image_id += 1\n",
    "\n",
    "    coco_dict = {\n",
    "        'images': images,\n",
    "        'annotations': annotations,\n",
    "        'categories': categories\n",
    "    }\n",
    "\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(coco_dict, f)\n",
    "\n",
    "# Convert train and valid sets\n",
    "yolo_to_coco('/kaggle/input/traffic-dataset/traffic_wala_dataset/train', 'train_coco.json')\n",
    "yolo_to_coco('/kaggle/input/traffic-dataset/traffic_wala_dataset/valid', 'valid_coco.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T13:51:56.228957Z",
     "iopub.status.busy": "2025-06-15T13:51:56.228377Z",
     "iopub.status.idle": "2025-06-15T13:53:13.655450Z",
     "shell.execute_reply": "2025-06-15T13:53:13.654706Z",
     "shell.execute_reply.started": "2025-06-15T13:51:56.228918Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m855.8/855.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.6/165.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install lightly --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T13:53:13.656864Z",
     "iopub.status.busy": "2025-06-15T13:53:13.656619Z",
     "iopub.status.idle": "2025-06-15T13:53:13.851581Z",
     "shell.execute_reply": "2025-06-15T13:53:13.850637Z",
     "shell.execute_reply.started": "2025-06-15T13:53:13.656827Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images available for SSVL pretraining: 626\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Define image paths\n",
    "train_images = glob.glob(\"/kaggle/input/traffic-dataset/traffic_wala_dataset/train/images/*.jpg\")\n",
    "val_images = glob.glob(\"/kaggle/input/traffic-dataset/traffic_wala_dataset/valid/images/*.jpg\")\n",
    "\n",
    "all_images = train_images + val_images\n",
    "print(f\"Total images available for SSVL pretraining: {len(all_images)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simsiam creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T13:53:17.037439Z",
     "iopub.status.busy": "2025-06-15T13:53:17.036727Z",
     "iopub.status.idle": "2025-06-15T13:53:23.406104Z",
     "shell.execute_reply": "2025-06-15T13:53:23.405505Z",
     "shell.execute_reply.started": "2025-06-15T13:53:17.037417Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "class SimSiamTrafficDataset(Dataset):\n",
    "    def __init__(self, image_paths):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomApply([transforms.ColorJitter()], p=0.8),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        return self.transform(img), self.transform(img)  # two views\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T13:53:23.407543Z",
     "iopub.status.busy": "2025-06-15T13:53:23.407172Z",
     "iopub.status.idle": "2025-06-15T13:53:23.411646Z",
     "shell.execute_reply": "2025-06-15T13:53:23.410909Z",
     "shell.execute_reply.started": "2025-06-15T13:53:23.407519Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = SimSiamTrafficDataset(all_images)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimSiam model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T13:53:24.020202Z",
     "iopub.status.busy": "2025-06-15T13:53:24.019473Z",
     "iopub.status.idle": "2025-06-15T13:53:29.817275Z",
     "shell.execute_reply": "2025-06-15T13:53:29.816300Z",
     "shell.execute_reply.started": "2025-06-15T13:53:24.020178Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 163MB/s] \n",
      "/usr/local/lib/python3.11/dist-packages/lightly/models/simsiam.py:65: Warning: The high-level building block SimSiam will be deprecated in version 1.3.0. Use low-level building blocks instead. See https://docs.lightly.ai/self-supervised-learning/lightly.models.html for more information\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import lightly.models as models\n",
    "import lightly.loss as loss\n",
    "import lightly.models.modules.heads as heads\n",
    "import torchvision.models as tv_models\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResNet18Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = tv_models.resnet18(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1])  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)           \n",
    "        return x.view(x.size(0), -1)   \n",
    "\n",
    "backbone = ResNet18Backbone()\n",
    "\n",
    "simsiam_model = models.SimSiam(\n",
    "    backbone,\n",
    "    num_ftrs=512,            \n",
    "    proj_hidden_dim=2048,\n",
    "    pred_hidden_dim=512,\n",
    "    out_dim=2048\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Simsiam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T13:53:32.964390Z",
     "iopub.status.busy": "2025-06-15T13:53:32.963732Z",
     "iopub.status.idle": "2025-06-15T13:56:31.735518Z",
     "shell.execute_reply": "2025-06-15T13:56:31.734440Z",
     "shell.execute_reply.started": "2025-06-15T13:53:32.964365Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/lightly/loss/sym_neg_cos_sim_loss.py:40: Warning: SymNegCosineSimiliarityLoss will be deprecated in favor of NegativeCosineSimilarity in the future.\n",
      "  warnings.warn(\n",
      "Epoch 1/50: 100%|██████████| 20/20 [00:05<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: -0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 20/20 [00:03<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: -0.1865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 20/20 [00:03<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: -0.4532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 20/20 [00:03<00:00,  5.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: -0.6634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 20/20 [00:03<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: -0.7612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 20/20 [00:03<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: -0.8037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 20/20 [00:03<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: -0.8364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 20/20 [00:03<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: -0.8502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 20/20 [00:03<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: -0.8686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 20/20 [00:03<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: -0.8822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 20/20 [00:03<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: -0.8939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 20/20 [00:03<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: -0.9046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 20/20 [00:03<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: -0.8966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 20/20 [00:03<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: -0.9094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 20/20 [00:03<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: -0.9036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 20/20 [00:03<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: -0.9114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 20/20 [00:03<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: -0.9144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 20/20 [00:03<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: -0.9230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 20/20 [00:03<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: -0.9228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 20/20 [00:03<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: -0.9303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 20/20 [00:03<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Loss: -0.9316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 20/20 [00:03<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Loss: -0.9393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 20/20 [00:03<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Loss: -0.9423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 20/20 [00:03<00:00,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Loss: -0.9402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 20/20 [00:03<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Loss: -0.9421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 20/20 [00:03<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Loss: -0.9446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 20/20 [00:03<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Loss: -0.9427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 20/20 [00:03<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Loss: -0.9453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 20/20 [00:03<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Loss: -0.9454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 20/20 [00:04<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Loss: -0.9475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 20/20 [00:03<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Loss: -0.9474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 20/20 [00:03<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Loss: -0.9514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 20/20 [00:03<00:00,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Loss: -0.9523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 20/20 [00:03<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Loss: -0.9491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 20/20 [00:03<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Loss: -0.9513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 20/20 [00:03<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Loss: -0.9524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 20/20 [00:03<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Loss: -0.9574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 20/20 [00:03<00:00,  5.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Loss: -0.9518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████| 20/20 [00:03<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Loss: -0.9579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████| 20/20 [00:03<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Loss: -0.9566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 20/20 [00:03<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, Loss: -0.9551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████| 20/20 [00:03<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, Loss: -0.9560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|██████████| 20/20 [00:03<00:00,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43, Loss: -0.9568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|██████████| 20/20 [00:03<00:00,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, Loss: -0.9619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|██████████| 20/20 [00:03<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, Loss: -0.9623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|██████████| 20/20 [00:03<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, Loss: -0.9588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|██████████| 20/20 [00:03<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, Loss: -0.9607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|██████████| 20/20 [00:03<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, Loss: -0.9606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|██████████| 20/20 [00:03<00:00,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, Loss: -0.9547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████| 20/20 [00:03<00:00,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: -0.9635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import SGD\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "simsiam_model = simsiam_model.to(device)\n",
    "\n",
    "criterion = loss.SymNegCosineSimilarityLoss()\n",
    "optimizer = SGD(simsiam_model.parameters(), lr=0.05, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "epochs = 50  \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    simsiam_model.train()\n",
    "    \n",
    "    for (x0, x1) in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        x0, x1 = x0.to(device), x1.to(device)\n",
    "        out0, out1 = simsiam_model(x0, x1)\n",
    "        loss_val = criterion(out0, out1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss_val.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T13:56:36.303494Z",
     "iopub.status.busy": "2025-06-15T13:56:36.303193Z",
     "iopub.status.idle": "2025-06-15T13:56:36.381452Z",
     "shell.execute_reply": "2025-06-15T13:56:36.380833Z",
     "shell.execute_reply.started": "2025-06-15T13:56:36.303467Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoder_state_dict = simsiam_model.backbone.state_dict()\n",
    "torch.save(encoder_state_dict, \"resnet18_simsiam_encoder.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COCO file create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T13:57:41.280549Z",
     "iopub.status.busy": "2025-06-15T13:57:41.279799Z",
     "iopub.status.idle": "2025-06-15T13:57:43.386134Z",
     "shell.execute_reply": "2025-06-15T13:57:43.385353Z",
     "shell.execute_reply.started": "2025-06-15T13:57:41.280523Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "img_dir = Path(\"/kaggle/input/traffic-dataset/traffic_wala_dataset/train/images\")\n",
    "label_dir = Path(\"/kaggle/input/traffic-dataset/traffic_wala_dataset/train/labels\")\n",
    "output_json = \"train_coco.json\"\n",
    "\n",
    "coco = {\n",
    "    \"images\": [],\n",
    "    \"annotations\": [],\n",
    "    \"categories\": [{\"id\": 0, \"name\": \"vehicle\"}],\n",
    "}\n",
    "\n",
    "ann_id = 1\n",
    "for img_id, img_file in enumerate(sorted(img_dir.glob(\"*.jpg\"))):\n",
    "    img = Image.open(img_file)\n",
    "    width, height = img.size\n",
    "    img_name = img_file.name\n",
    "    coco[\"images\"].append({\n",
    "        \"id\": img_id,\n",
    "        \"file_name\": img_name,\n",
    "        \"width\": width,\n",
    "        \"height\": height\n",
    "    })\n",
    "\n",
    "    \n",
    "    label_file = label_dir / img_file.with_suffix('.txt').name\n",
    "    if not label_file.exists():\n",
    "        continue\n",
    "\n",
    "    with open(label_file, 'r') as f:\n",
    "        for line in f:\n",
    "            cls, x_center, y_center, w, h = map(float, line.strip().split())\n",
    "\n",
    "            \n",
    "            x = (x_center - w / 2) * width\n",
    "            y = (y_center - h / 2) * height\n",
    "            box_w = w * width\n",
    "            box_h = h * height\n",
    "\n",
    "            coco[\"annotations\"].append({\n",
    "                \"id\": ann_id,\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": 0,\n",
    "                \"bbox\": [x, y, box_w, box_h],\n",
    "                \"area\": box_w * box_h,\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            ann_id += 1\n",
    "\n",
    "with open(output_json, 'w') as f:\n",
    "    json.dump(coco, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T13:57:50.054479Z",
     "iopub.status.busy": "2025-06-15T13:57:50.053728Z",
     "iopub.status.idle": "2025-06-15T13:57:50.445667Z",
     "shell.execute_reply": "2025-06-15T13:57:50.445136Z",
     "shell.execute_reply.started": "2025-06-15T13:57:50.054453Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "img_dir = Path(\"/kaggle/input/traffic-dataset/traffic_wala_dataset/valid/images\")\n",
    "label_dir = Path(\"/kaggle/input/traffic-dataset/traffic_wala_dataset/valid/labels\")\n",
    "output_json = \"val_coco.json\"\n",
    "\n",
    "coco = {\n",
    "    \"images\": [],\n",
    "    \"annotations\": [],\n",
    "    \"categories\": [{\"id\": 0, \"name\": \"vehicle\"}],\n",
    "}\n",
    "\n",
    "ann_id = 1\n",
    "for img_id, img_file in enumerate(sorted(img_dir.glob(\"*.jpg\"))):\n",
    "    img = Image.open(img_file)\n",
    "    width, height = img.size\n",
    "    img_name = img_file.name\n",
    "    coco[\"images\"].append({\n",
    "        \"id\": img_id,\n",
    "        \"file_name\": img_name,\n",
    "        \"width\": width,\n",
    "        \"height\": height\n",
    "    })\n",
    "\n",
    "    \n",
    "    label_file = label_dir / img_file.with_suffix('.txt').name\n",
    "    if not label_file.exists():\n",
    "        continue\n",
    "\n",
    "    with open(label_file, 'r') as f:\n",
    "        for line in f:\n",
    "            cls, x_center, y_center, w, h = map(float, line.strip().split())\n",
    "\n",
    "            \n",
    "            x = (x_center - w / 2) * width\n",
    "            y = (y_center - h / 2) * height\n",
    "            box_w = w * width\n",
    "            box_h = h * height\n",
    "\n",
    "            coco[\"annotations\"].append({\n",
    "                \"id\": ann_id,\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": 0,\n",
    "                \"bbox\": [x, y, box_w, box_h],\n",
    "                \"area\": box_w * box_h,\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            ann_id += 1\n",
    "\n",
    "\n",
    "with open(output_json, 'w') as f:\n",
    "    json.dump(coco, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T13:58:24.374236Z",
     "iopub.status.busy": "2025-06-15T13:58:24.373447Z",
     "iopub.status.idle": "2025-06-15T13:58:27.715101Z",
     "shell.execute_reply": "2025-06-15T13:58:27.714042Z",
     "shell.execute_reply.started": "2025-06-15T13:58:24.374209Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q pycocotools torchvision albumentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T14:00:27.515347Z",
     "iopub.status.busy": "2025-06-15T14:00:27.515068Z",
     "iopub.status.idle": "2025-06-15T14:00:27.749149Z",
     "shell.execute_reply": "2025-06-15T14:00:27.748545Z",
     "shell.execute_reply.started": "2025-06-15T14:00:27.515329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as tv_models\n",
    "\n",
    "\n",
    "class ResNet18Backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = tv_models.resnet18(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-1]) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)           \n",
    "        return x.view(x.size(0), -1)   \n",
    "\n",
    "\n",
    "model = ResNet18Backbone()\n",
    "\n",
    "state_dict = torch.load(\"/kaggle/working/resnet18_simsiam_encoder.pth\", map_location=\"cpu\")\n",
    "model.load_state_dict(state_dict) \n",
    "\n",
    "\n",
    "encoder = model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T14:00:33.161879Z",
     "iopub.status.busy": "2025-06-15T14:00:33.161585Z",
     "iopub.status.idle": "2025-06-15T14:00:33.165589Z",
     "shell.execute_reply": "2025-06-15T14:00:33.164993Z",
     "shell.execute_reply.started": "2025-06-15T14:00:33.161839Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T14:00:53.708465Z",
     "iopub.status.busy": "2025-06-15T14:00:53.708203Z",
     "iopub.status.idle": "2025-06-15T14:00:53.924214Z",
     "shell.execute_reply": "2025-06-15T14:00:53.923594Z",
     "shell.execute_reply.started": "2025-06-15T14:00:53.708448Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision.models._utils import IntermediateLayerGetter\n",
    "from torchvision.models.detection.backbone_utils import BackboneWithFPN\n",
    "\n",
    "return_layers = {\n",
    "    'layer1': '0', \n",
    "    'layer2': '1',  \n",
    "    'layer3': '2',  \n",
    "    'layer4': '3'   \n",
    "}\n",
    "body = IntermediateLayerGetter(tv_models.resnet18(pretrained=True), return_layers=return_layers)\n",
    "\n",
    "fpn_backbone = BackboneWithFPN(\n",
    "    backbone=body,\n",
    "    return_layers=return_layers,\n",
    "    in_channels_list=[64, 128, 256, 512],\n",
    "    out_channels=256\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T14:00:56.421019Z",
     "iopub.status.busy": "2025-06-15T14:00:56.420566Z",
     "iopub.status.idle": "2025-06-15T14:00:56.515305Z",
     "shell.execute_reply": "2025-06-15T14:00:56.514536Z",
     "shell.execute_reply.started": "2025-06-15T14:00:56.420998Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection import RetinaNet\n",
    "\n",
    "model = RetinaNet(\n",
    "    backbone=fpn_backbone,\n",
    "    num_classes=2  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T14:00:58.508313Z",
     "iopub.status.busy": "2025-06-15T14:00:58.508026Z",
     "iopub.status.idle": "2025-06-15T14:00:58.512366Z",
     "shell.execute_reply": "2025-06-15T14:00:58.511699Z",
     "shell.execute_reply.started": "2025-06-15T14:00:58.508294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for param in model.backbone.body.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T14:01:01.902388Z",
     "iopub.status.busy": "2025-06-15T14:01:01.901671Z",
     "iopub.status.idle": "2025-06-15T14:01:01.909486Z",
     "shell.execute_reply": "2025-06-15T14:01:01.908670Z",
     "shell.execute_reply.started": "2025-06-15T14:01:01.902364Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision import transforms as T\n",
    "import os\n",
    "\n",
    "class COCODetectionDataset(CocoDetection):\n",
    "    def __init__(self, img_folder, ann_file, transforms=None):\n",
    "        super().__init__(img_folder, ann_file)\n",
    "        self._transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = super().__getitem__(idx)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {'boxes': [], 'labels': [], 'image_id': image_id}\n",
    "\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        for ann in anns:\n",
    "            bbox = ann['bbox']  \n",
    "            x, y, w, h = bbox\n",
    "            target['boxes'].append([x, y, x + w, y + h])\n",
    "            target['labels'].append(ann['category_id'])\n",
    "\n",
    "        target['boxes'] = torch.tensor(target['boxes'], dtype=torch.float32)\n",
    "        target['labels'] = torch.tensor(target['labels'], dtype=torch.int64)\n",
    "        target['image_id'] = torch.tensor([image_id])\n",
    "\n",
    "        if self._transforms:\n",
    "            img = self._transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T14:01:03.963140Z",
     "iopub.status.busy": "2025-06-15T14:01:03.962831Z",
     "iopub.status.idle": "2025-06-15T14:01:04.013242Z",
     "shell.execute_reply": "2025-06-15T14:01:04.012569Z",
     "shell.execute_reply.started": "2025-06-15T14:01:03.963118Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "transform = T.Compose([\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = COCODetectionDataset(\n",
    "    img_folder='/kaggle/input/traffic-dataset/traffic_wala_dataset/train/images',\n",
    "    ann_file='/kaggle/input/coco-format/train_coco.json',\n",
    "    transforms=transform\n",
    ")\n",
    "\n",
    "val_dataset = COCODetectionDataset(\n",
    "    img_folder='/kaggle/input/traffic-dataset/traffic_wala_dataset/valid/images',\n",
    "    ann_file='/kaggle/input/coco-format/val_coco.json',\n",
    "    transforms=transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T14:01:06.291553Z",
     "iopub.status.busy": "2025-06-15T14:01:06.291046Z",
     "iopub.status.idle": "2025-06-15T14:01:06.296132Z",
     "shell.execute_reply": "2025-06-15T14:01:06.295310Z",
     "shell.execute_reply.started": "2025-06-15T14:01:06.291533Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T14:01:08.917940Z",
     "iopub.status.busy": "2025-06-15T14:01:08.917237Z",
     "iopub.status.idle": "2025-06-15T14:01:08.925904Z",
     "shell.execute_reply": "2025-06-15T14:01:08.925303Z",
     "shell.execute_reply.started": "2025-06-15T14:01:08.917916Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10):\n",
    "    model.train()\n",
    "    lr_scheduler = None\n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1.0 / 1000\n",
    "        warmup_iters = min(1000, len(data_loader) - 1)\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
    "        )\n",
    "\n",
    "    pbar = tqdm(enumerate(data_loader), total=len(data_loader), desc=f\"Epoch {epoch}\")\n",
    "    for i, (images, targets) in pbar:\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        pbar.set_postfix(loss=losses.item())\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    pbar = tqdm(data_loader, desc=\"Evaluating\")\n",
    "    for images, targets in pbar:\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        outputs = model(images)\n",
    "        total += len(outputs)\n",
    "    print(f\"✅ Evaluation done. Processed {total} images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T14:01:11.753397Z",
     "iopub.status.busy": "2025-06-15T14:01:11.753130Z",
     "iopub.status.idle": "2025-06-15T15:08:08.421206Z",
     "shell.execute_reply": "2025-06-15T15:08:08.420357Z",
     "shell.execute_reply.started": "2025-06-15T14:01:11.753377Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 134/134 [01:14<00:00,  1.80it/s, loss=1.02] \n",
      "Evaluating: 100%|██████████| 45/45 [00:06<00:00,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: 100%|██████████| 134/134 [01:14<00:00,  1.80it/s, loss=0.991]\n",
      "Evaluating: 100%|██████████| 45/45 [00:06<00:00,  7.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: 100%|██████████| 134/134 [01:14<00:00,  1.80it/s, loss=0.889]\n",
      "Evaluating: 100%|██████████| 45/45 [00:06<00:00,  7.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: 100%|██████████| 134/134 [01:14<00:00,  1.80it/s, loss=0.891]\n",
      "Evaluating: 100%|██████████| 45/45 [00:06<00:00,  7.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.685]\n",
      "Evaluating: 100%|██████████| 45/45 [00:06<00:00,  7.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: 100%|██████████| 134/134 [01:14<00:00,  1.80it/s, loss=0.816]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.582]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.598]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.603]\n",
      "Evaluating: 100%|██████████| 45/45 [00:06<00:00,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.631]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.428]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.539]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.491]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.407]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.487]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.376]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.361]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17: 100%|██████████| 134/134 [01:14<00:00,  1.80it/s, loss=0.337]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.376]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.402]\n",
      "Evaluating: 100%|██████████| 45/45 [00:06<00:00,  7.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20: 100%|██████████| 134/134 [01:14<00:00,  1.80it/s, loss=0.32] \n",
      "Evaluating: 100%|██████████| 45/45 [00:06<00:00,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21: 100%|██████████| 134/134 [01:14<00:00,  1.80it/s, loss=0.341]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.286]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.258]\n",
      "Evaluating: 100%|██████████| 45/45 [00:06<00:00,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.284]\n",
      "Evaluating: 100%|██████████| 45/45 [00:06<00:00,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.29] \n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.265]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.235]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28: 100%|██████████| 134/134 [01:14<00:00,  1.80it/s, loss=0.271]\n",
      "Evaluating: 100%|██████████| 45/45 [00:06<00:00,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29: 100%|██████████| 134/134 [01:14<00:00,  1.80it/s, loss=0.247]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30: 100%|██████████| 134/134 [01:14<00:00,  1.80it/s, loss=0.236]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.231]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.19] \n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.238]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.22] \n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35: 100%|██████████| 134/134 [01:14<00:00,  1.80it/s, loss=0.24] \n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|██████████| 134/134 [01:14<00:00,  1.80it/s, loss=0.227]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.179]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.217]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39: 100%|██████████| 134/134 [01:14<00:00,  1.80it/s, loss=0.248]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40: 100%|██████████| 134/134 [01:14<00:00,  1.81it/s, loss=0.204]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|██████████| 134/134 [01:14<00:00,  1.80it/s, loss=0.262]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42: 100%|██████████| 134/134 [01:14<00:00,  1.80it/s, loss=0.147]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43: 100%|██████████| 134/134 [01:14<00:00,  1.80it/s, loss=0.191]\n",
      "Evaluating: 100%|██████████| 45/45 [00:06<00:00,  7.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44: 100%|██████████| 134/134 [01:14<00:00,  1.80it/s, loss=0.272]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45: 100%|██████████| 134/134 [01:14<00:00,  1.80it/s, loss=0.179]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46: 100%|██████████| 134/134 [01:14<00:00,  1.80it/s, loss=0.173]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47: 100%|██████████| 134/134 [01:14<00:00,  1.80it/s, loss=0.173]\n",
      "Evaluating: 100%|██████████| 45/45 [00:06<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|██████████| 134/134 [01:14<00:00,  1.80it/s, loss=0.179]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49: 100%|██████████| 134/134 [01:14<00:00,  1.80it/s, loss=0.171]\n",
      "Evaluating: 100%|██████████| 45/45 [00:05<00:00,  7.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation done. Processed 90 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "    evaluate(model, val_loader, device)\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"retinanet_finetuned_ssvl.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T15:08:46.936705Z",
     "iopub.status.busy": "2025-06-15T15:08:46.936056Z",
     "iopub.status.idle": "2025-06-15T15:08:54.007062Z",
     "shell.execute_reply": "2025-06-15T15:08:54.006133Z",
     "shell.execute_reply.started": "2025-06-15T15:08:46.936671Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing:   4%|▍         | 2/45 [00:00<00:10,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Debug: outputs[0] keys and shapes ===\n",
      " boxes: torch.Size([51, 4])\n",
      " scores: torch.Size([51])\n",
      " labels: torch.Size([51])\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing: 100%|██████████| 45/45 [00:05<00:00,  7.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total detections collected: 3138\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.57s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.580\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.915\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.674\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.457\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.620\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.590\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.066\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.446\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.658\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.569\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.690\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.654\n"
     ]
    }
   ],
   "source": [
    "with open(\"val_coco.json\", \"r\") as f:\n",
    "    coco = json.load(f)\n",
    "coco[\"categories\"][0][\"id\"] = 1\n",
    "for ann in coco[\"annotations\"]:\n",
    "    ann[\"category_id\"] = 1\n",
    "with open(\"val_coco_fixed.json\", \"w\") as f:\n",
    "    json.dump(coco, f)\n",
    "\n",
    "class COCODetectionDataset(CocoDetection):\n",
    "    def __init__(self, img_folder, ann_file, transforms=None):\n",
    "        super().__init__(img_folder, ann_file)\n",
    "        self._transforms = transforms\n",
    "    def __getitem__(self, idx):\n",
    "        img, anns = super().__getitem__(idx)\n",
    "        image_id = self.ids[idx]\n",
    "        boxes, labels = [], []\n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann[\"category_id\"]) \n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor([image_id])\n",
    "        }\n",
    "        if self._transforms:\n",
    "            img = self._transforms(img)\n",
    "        return img, target\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Build FPN backbone + RetinaNet\n",
    "return_layers = {\"layer1\":\"0\",\"layer2\":\"1\",\"layer3\":\"2\",\"layer4\":\"3\"}\n",
    "body = IntermediateLayerGetter(\n",
    "    tv_models.resnet18(pretrained=False),\n",
    "    return_layers=return_layers\n",
    ")\n",
    "fpn_backbone = BackboneWithFPN(\n",
    "    backbone=body,\n",
    "    return_layers=return_layers,\n",
    "    in_channels_list=[64,128,256,512],\n",
    "    out_channels=256\n",
    ")\n",
    "\n",
    "model = RetinaNet(backbone=fpn_backbone, num_classes=2)  \n",
    "model.load_state_dict(torch.load(\"retinanet_finetuned_ssvl.pth\", map_location=\"cpu\"))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "val_ds = COCODetectionDataset(\n",
    "    img_folder=\"/kaggle/input/traffic-dataset/traffic_wala_dataset/valid/images\",\n",
    "    ann_file=\"val_coco_fixed.json\",\n",
    "    transforms=transform\n",
    ")\n",
    "val_loader = DataLoader(val_ds, batch_size=2, shuffle=False,\n",
    "                        num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (images, targets) in enumerate(tqdm(val_loader, desc=\"Inferencing\")):\n",
    "        imgs = [img.to(device) for img in images]\n",
    "        outs = model(imgs)\n",
    "\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            print(\"=== Debug: outputs[0] keys and shapes ===\")\n",
    "            for k, v in outs[0].items():\n",
    "                print(f\" {k}: {v.shape}\")\n",
    "            print(\"========================================\")\n",
    "\n",
    "        for out, tgt in zip(outs, targets):\n",
    "            img_id = int(tgt[\"image_id\"].item())\n",
    "            boxes = out[\"boxes\"].cpu().tolist()\n",
    "            scores = out[\"scores\"].cpu().tolist()\n",
    "            labels = out[\"labels\"].cpu().tolist()\n",
    "            for box, score, label in zip(boxes, scores, labels):\n",
    "                predictions.append({\n",
    "                    \"image_id\": img_id,\n",
    "                    \"category_id\": label + 1,\n",
    "                    \"bbox\": [box[0], box[1], box[2]-box[0], box[3]-box[1]],\n",
    "                    \"score\": score\n",
    "                })\n",
    "\n",
    "print(f\"Total detections collected: {len(predictions)}\")\n",
    "with open(\"predictions.json\", \"w\") as f:\n",
    "    json.dump(predictions, f)\n",
    "\n",
    "\n",
    "if len(predictions) == 0:\n",
    "    raise RuntimeError(\"No detections found! Check debug output above to see why `outs` might be empty.\")\n",
    "else:\n",
    "    coco_gt = COCO(\"val_coco_fixed.json\")\n",
    "    coco_dt = coco_gt.loadRes(\"predictions.json\")\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType=\"bbox\")\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T16:51:30.945414Z",
     "iopub.status.busy": "2025-06-15T16:51:30.945130Z",
     "iopub.status.idle": "2025-06-15T16:51:31.095636Z",
     "shell.execute_reply": "2025-06-15T16:51:31.095086Z",
     "shell.execute_reply.started": "2025-06-15T16:51:30.945393Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(scores, bins=20, range=(0, 1), color='skyblue')\n",
    "plt.title(\"Distribution of Detection Scores\")\n",
    "plt.xlabel(\"Confidence\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T16:56:18.249558Z",
     "iopub.status.busy": "2025-06-15T16:56:18.249044Z",
     "iopub.status.idle": "2025-06-15T16:56:18.258425Z",
     "shell.execute_reply": "2025-06-15T16:56:18.257756Z",
     "shell.execute_reply.started": "2025-06-15T16:56:18.249536Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(708.4714, 600.9456787109375, 'Class 0 0.06')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors = {1: 'red', 2: 'blue', 3: 'green'}  # extend as needed\n",
    "label_names = {1: \"Vehicle\"}\n",
    "\n",
    "# In loop:\n",
    "color = colors.get(label, \"white\")\n",
    "name = label_names.get(label, f\"Class {label}\")\n",
    "ax.add_patch(patches.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                               linewidth=2, edgecolor=color, facecolor=\"none\"))\n",
    "ax.text(x1, y1 - 10, f\"{name} {score:.2f}\", color=\"white\",\n",
    "        fontsize=12, weight=\"bold\",\n",
    "        bbox=dict(facecolor=color, edgecolor=\"none\", boxstyle=\"round,pad=0.3\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-15T17:34:01.295553Z",
     "iopub.status.busy": "2025-06-15T17:34:01.294991Z",
     "iopub.status.idle": "2025-06-15T17:35:54.696705Z",
     "shell.execute_reply": "2025-06-15T17:35:54.696057Z",
     "shell.execute_reply.started": "2025-06-15T17:34:01.295528Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Detection complete! Output saved as 'output_detected.mp4'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "from torchvision.models.detection import RetinaNet\n",
    "from torchvision.models.detection.backbone_utils import BackboneWithFPN\n",
    "from torchvision.models._utils import IntermediateLayerGetter\n",
    "import torchvision.models as tv_models\n",
    "from PIL import Image\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load RetinaNet model\n",
    "return_layers = {\"layer1\": \"0\", \"layer2\": \"1\", \"layer3\": \"2\", \"layer4\": \"3\"}\n",
    "body = IntermediateLayerGetter(\n",
    "    tv_models.resnet18(pretrained=False),\n",
    "    return_layers=return_layers\n",
    ")\n",
    "fpn_backbone = BackboneWithFPN(\n",
    "    backbone=body,\n",
    "    return_layers=return_layers,\n",
    "    in_channels_list=[64, 128, 256, 512],\n",
    "    out_channels=256\n",
    ")\n",
    "model = RetinaNet(backbone=fpn_backbone, num_classes=2)\n",
    "model.load_state_dict(torch.load(\"/kaggle/input/final-model/pytorch/default/1/retinanet_finetuned_ssvl.pth\", map_location=device))\n",
    "model.to(device).eval()\n",
    "\n",
    "# Video settings\n",
    "video_path = \"/kaggle/input/vidtryyy/56310-479197605_small.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Save output\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('output_detected.mp4', fourcc, 30.0,\n",
    "                      (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "\n",
    "# Transform\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "threshold = 0.3  # Confidence threshold\n",
    "\n",
    "# Process video\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert frame to tensor\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_img = Image.fromarray(rgb_frame)\n",
    "    input_tensor = transform(pil_img).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model([input_tensor])[0]\n",
    "\n",
    "    boxes = output[\"boxes\"].cpu().numpy()\n",
    "    scores = output[\"scores\"].cpu().numpy()\n",
    "\n",
    "    # Draw boxes\n",
    "    for box, score in zip(boxes, scores):\n",
    "        if score < threshold:\n",
    "            continue\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        cv2.putText(frame, f\"Vehicle {score:.2f}\", (x1, y1 - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (36, 255, 12), 2)\n",
    "\n",
    "    out.write(frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"✅ Detection complete! Output saved as 'output_detected.mp4'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7640980,
     "sourceId": 12133461,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7666097,
     "sourceId": 12172052,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7668262,
     "sourceId": 12175672,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7668506,
     "sourceId": 12176030,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 376987,
     "modelInstanceId": 355687,
     "sourceId": 436107,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
